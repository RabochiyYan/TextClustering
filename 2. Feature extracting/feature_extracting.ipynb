{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980a9879-4e68-4b8b-a137-3a036bb609bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d07a81-4e03-4592-b593-189c70d3f149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ученый', 'который', 'совершать', 'революция'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['великий', 'император', 'эфир', 'матч', 'тв',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['апрель', 'пройти', 'акция', 'монетный', 'нед...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['азия', 'никуда', 'полет', 'иркутск', 'владив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ия', 'экран', 'появляться', 'первый', 'подро...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  ['ученый', 'который', 'совершать', 'революция'...\n",
       "1  ['великий', 'император', 'эфир', 'матч', 'тв',...\n",
       "2  ['апрель', 'пройти', 'акция', 'монетный', 'нед...\n",
       "3  ['азия', 'никуда', 'полет', 'иркутск', 'владив...\n",
       "4  ['ия', 'экран', 'появляться', 'первый', 'подро..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../1. Preprocessing /data_preprocessed.csv').drop(columns=['Unnamed: 0'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54595169-95d9-4384-947d-f80ad02aad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['ученый', 'который', 'совершать', 'революция', 'лечение', 'рассеянный', 'склероз', 'получать', 'научный', 'оскар']\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['0'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7e90da-1b86-4f49-bd81-ec031839d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [ученый, который, совершать, революция, лечени...\n",
       "1        [великий, император, эфир, матч, тв, интервью,...\n",
       "2        [апрель, пройти, акция, монетный, неделя, росс...\n",
       "3        [азия, никуда, полет, иркутск, владивосток, ко...\n",
       "4        [ия, экран, появляться, первый, подробность, с...\n",
       "                               ...                        \n",
       "16217    [согласовывать, отпуск, наш, счет, возвращать,...\n",
       "16218                                    [рубрика, директ]\n",
       "16219    [рома, опыт, камингаут, сравнительно, небольшо...\n",
       "16220    [гранд, должный, обыгрывать, аутсайдер, кубок,...\n",
       "16221                                    [главное, крипта]\n",
       "Name: 0, Length: 16222, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data['0'].apply(lambda x: x.strip('[\\']').split('\\', \\''))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b83428-e64a-42b4-b0c7-a367afac62ea",
   "metadata": {},
   "source": [
    "# 2. Извлечение признаков:\n",
    "\n",
    "После предварительной обработки наступает время преобразования текстовых данных в числовой формат, подходящий для кластеризации.  \n",
    "Это можно сделать любым из следующих популярных способов:\n",
    "\n",
    "- 2.1 **«Мешок слов» (BoW)**: представление текстов в виде группы слов и их частоты\n",
    "- 2.2 **Tермин «Частота»:** обратная частота документа (TF-IDF) — это показатель, который показывает, насколько значимо данное слово в конкретном документе по сравнению с другими документами\n",
    "- 2.3 **Использование предварительно обученных моделей:** таких как Word2Vec (Миколов и др., 2013), GloVe и BERT для представления слов в виде плотных векторов, отражающих семантические связи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26810931-a77e-44bb-af0e-8506e4247555",
   "metadata": {},
   "source": [
    "## 2.1 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f0593-c057-43ca-93be-0893b173e5a6",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b48ea6d-4a25-4707-9db3-2e069a82e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # Список слов (фич)\n",
    "print(X.toarray())  # Матрица \"документ × слово\" (количество вхождений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dceb7458-177e-4235-8272-10605c9bc2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ученый который совершать революция лечение рассеянный склероз получать научный оскар',\n",
       " 'великий император эфир матч тв интервью сам франческо тотти легендарный римлянин приезжать церемония премия рб который пройти завтра заглядывать мы гость вопрос черданцев ответ тотти ловить эфир матч',\n",
       " 'апрель пройти акция монетный неделя россиянин смочь бесплатно обменять монета банкнота зачислять сумма счет сообщать центробанк обмен доступный тыс отделение банк тыс торговый точка весь страна',\n",
       " 'азия никуда полет иркутск владивосток корея индонезия малайзия вьетнам филиппины багаж рубль тудаобратно везти никакой виза вылет маеиюнь',\n",
       " 'ия экран появляться первый подробность смартфон бывший главный дизайнер джонни айва известно девайс антисмартфон получать голосовой управление экран заточить работа ия быть функция смартфон источник пока мочь придумывать сравнивать гаджет ждать анонс']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn осуществляет препроцессинг по умолчанию, поэтому придется соединить массивы в строку\n",
    "\n",
    "data_test = data.copy(deep=True)\n",
    "data_test = data_test.apply(lambda x: ' '.join(x))\n",
    "data_test = list(data_test.values)\n",
    "data_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d477aa-d271-468b-a32a-61f133d34bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аа' 'ааа' 'аааа' ... 'ящик' 'яя' 'ён']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[10 28 26 ... 73 69  2]\n",
      "[10, 28, 26] [74, 69, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "X = vectorizer.fit_transform(data_test)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # Список слов (фич)\n",
    "print(X.toarray())  # Матрица \"документ × слово\" (количество вхождений)\n",
    "print(X.toarray().sum(axis=1)) # Считаем общее количество вхождений для каждого объекта\n",
    "print(list(data.apply(lambda x: len(x)))[0:3], list(data.apply(lambda x: len(x)))[-3:]) # Считаем изначальное количество слов у каждого объекта выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4fad5d0-2a85-4751-8308-26a294cf0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16222 16222\n"
     ]
    }
   ],
   "source": [
    "data_lens = data.apply(lambda x: len(x))\n",
    "x_lens = X.toarray().sum(axis=1)\n",
    "print(len(data_lens), len(x_lens))\n",
    "#Количество объектов не изменилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "189bd672-0173-49ac-90f7-cc711990b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007\n"
     ]
    }
   ],
   "source": [
    "unmatch_count = 0\n",
    "for i, data_len in enumerate(data_lens):\n",
    "    x_len = x_lens[i]\n",
    "    if data_len != x_len:\n",
    "        unmatch_count+=1\n",
    "\n",
    "print(unmatch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218f4e9-2e8b-4f37-8710-9baa04792d37",
   "metadata": {},
   "source": [
    "Длина слов и сумма строки матрицы не сходятся у некоторых объектов, по всей видимости список стоп-слов, использованных ранее, отличается от используемого в sklearn. Попроубем проверить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef21a1-3f65-45fa-9ffd-e1b010e34302",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "x_array = X.toarray()\n",
    "for i, data_len in enumerate(data_lens):\n",
    "    x_len = x_lens[i]\n",
    "    if data_len != x_len:\n",
    "        x_set = set()\n",
    "        data_set = set(data.values[i])\n",
    "\n",
    "        for j, feature in enumerate(features):\n",
    "            if x_array[i][j] == 1:\n",
    "                x_set.add(feature)\n",
    "        print(i)\n",
    "        print(f'Длина до обработки: {data_len}\\nДлина после обработки: {x_len}\\nРазница: {data_len-x_len}')\n",
    "        print(f'Длина до обработки без повторения: {len(data_set)}\\nДлина после обработки без повторения: {len(x_set)}\\nРазница: {len(data_set-x_set)}')\n",
    "        print(f'Слова до обработки: {data_set}')\n",
    "        print(f'Слова после обработки: {x_set}')\n",
    "        print(f'Удаленные слова: {data_set-x_set}')\n",
    "        assert len(x_set-data_set) == 0 # Проверка на то, что новые слова не добавились\n",
    "        print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05a9c3-4b2e-4e0d-b243-1411c32eda87",
   "metadata": {},
   "source": [
    "Как можно видеть хоть параметр stop_words = None, CountVectorizer все равно почему-то удаляет слова, причем чаще наполненных смыслом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5203146-4cb7-4968-8fd2-e476340344ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37248"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c7671-3330-402e-b427-2cc21929ee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8821bd-8b5e-4609-86ab-7dc201f8a4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07404c-d6c6-41a0-a13b-3152621d0296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3b99d-3108-4525-9f03-4ee49d1c3ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a95418-aa9d-49b4-a6e3-aa0e81fe8fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658f8f10-c60c-49a5-b1fa-17343a054804",
   "metadata": {},
   "source": [
    "## 2.2 Tермин «Частота»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c91ed-b786-478c-aa90-133c2d97f438",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75650012-9ac8-4d43-91e5-84b0c05a0c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Термины: ['второй' 'документ' 'ли' 'первый' 'третий' 'это' 'этот']\n",
      "Матрица TF-IDF:\n",
      " [[0.         0.4574528  0.         0.69113141 0.         0.55953044\n",
      "  0.        ]\n",
      " [0.66338461 0.34618161 0.         0.         0.         0.\n",
      "  0.66338461]\n",
      " [0.         0.40264194 0.         0.         0.77157901 0.49248889\n",
      "  0.        ]\n",
      " [0.         0.34399327 0.65919112 0.51971385 0.         0.42075315\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Пример документов\n",
    "documents = [\n",
    "    \"Это первый документ.\",\n",
    "    \"Этот документ второй.\",\n",
    "    \"И это третий документ.\",\n",
    "    \"Первый ли это документ?\",\n",
    "]\n",
    "\n",
    "# Создание векторизатора TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Вывод терминов и матрицы TF-IDF\n",
    "print(\"Термины:\", vectorizer.get_feature_names_out())\n",
    "print(\"Матрица TF-IDF:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed41b63f-4b36-4f3e-aca9-f9867bf20faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['второй', 'документ', 'ли', 'первый', 'третий', 'это', 'этот'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3cf39a-8d7d-4512-b58b-f5bb70725904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Термины: ['аа' 'ааа' 'аааа' ... 'ящик' 'яя' 'ён'] 37248\n",
      "Матрица TF-IDF:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Создание векторизатора TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data_test)\n",
    "\n",
    "# Вывод терминов и матрицы TF-IDF\n",
    "print(\"Термины:\", vectorizer.get_feature_names_out(), len(vectorizer.get_feature_names_out()))\n",
    "print(\"Матрица TF-IDF:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5073d-305c-42b6-a1e8-1d08a0db4827",
   "metadata": {},
   "source": [
    "## 2.3 Использование предварительно обученных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f09a8-bcf3-4125-af3e-7df03e91199b",
   "metadata": {},
   "source": [
    "### 2.3.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef07014-4d78-45f9-ab10-b97e6aa21302",
   "metadata": {},
   "source": [
    "По документации пробуем библиотеку на простом корпусе https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f30af137-392f-4207-8d5e-95f5b2bf2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор слова 'слова': [-0.00713402  0.00124788 -0.00718346 -0.00223246  0.00373751]...\n",
      "Похожие слова на 'преобразует': [('для', 0.25297728180885315), ('векторы', 0.17048844695091248), ('nlp', 0.1501290202140808)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Пример текстового корпуса (список предложений)\n",
    "sentences = [\n",
    "    \"Word2Vec преобразует слова в векторы.\",\n",
    "    \"Векторы сохраняют семантические связи между словами.\",\n",
    "    \"Word2Vec используется в NLP для word embedding.\",\n",
    "    \"Примеры включают машинное обучение и обработку естественного языка.\"\n",
    "]\n",
    "\n",
    "# Токенизация предложений\n",
    "tokenized_sentences = [simple_preprocess(sentence, deacc=True) for sentence in sentences]\n",
    "\n",
    "# Обучение модели Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,     # Размерность вектора\n",
    "    window=5,            # Размер окна контекста\n",
    "    min_count=1,          # Минимальная частота слова для учета\n",
    "    workers=4,            # Количество потоков\n",
    "    epochs=50            # Количество эпох обучения\n",
    ")\n",
    "\n",
    "# Получение вектора для слова\n",
    "word_vector = model.wv[\"преобразует\"]\n",
    "print(f\"Вектор слова 'слова': {word_vector[:5]}...\")  # Вывод первых 5 значений\n",
    "\n",
    "# Поиск похожих слов\n",
    "similar_words = model.wv.most_similar(\"преобразует\", topn=3)\n",
    "print(\"Похожие слова на 'преобразует':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38d26c-2c05-481c-bf57-2acb3daded45",
   "metadata": {},
   "source": [
    "Обучаем модель на нашем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b7e2839-2677-4351-a0a8-7628935efc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.copy(deep=True)\n",
    "\n",
    "# Обучение модели Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=data_test,\n",
    "    vector_size=100,     # Размерность вектора\n",
    "    window=5,            # Размер окна контекста\n",
    "    min_count=1,          # Минимальная частота слова для учета\n",
    "    workers=4,            # Количество потоков\n",
    "    epochs=50            # Количество эпох обучения\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff49235-f591-426d-8ff3-2575e1e65e6f",
   "metadata": {},
   "source": [
    "Проверяем работу модели смотря близки ли по смыслу близкие в полученном пространестве вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ea91537-4e9d-4647-adb3-4b1151340a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор слова 'чай': [-0.8308413   1.0660404   2.5957007  -0.7834279  -0.08392825]...\n",
      "Похожие слова на 'чай': [('овсянка', 0.6867110729217529), ('трава', 0.6736602783203125), ('ветчина', 0.6692957282066345)]\n"
     ]
    }
   ],
   "source": [
    "# Получение вектора для слова\n",
    "word_vector = model.wv[\"чай\"]\n",
    "print(f\"Вектор слова 'чай': {word_vector[:5]}...\")  # Вывод первых 5 значений\n",
    "\n",
    "# Поиск похожих слов\n",
    "similar_words = model.wv.most_similar(\"чай\", topn=3)\n",
    "print(\"Похожие слова на 'чай':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f62d0f-c1bb-4e81-9260-9da9f4dc4f5c",
   "metadata": {},
   "source": [
    "Здесь я хотел попробовать что-то похожее на знаменитый пример про короля и королеву, только с \"кофе\" и \"молоко\". Я сложил вектора этих слов и ожидал получить \"латте\", однако такого не вышло и кажется эти слова и так слишком близко друг к другу. Я пробовал на других словах, но хорошего результата не поучилось. Предполагаю, что это из-за того, что датасет довольно маленький."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f34d496d-5090-4865-867e-fb8a561e7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор слова 'молоко': [-0.8308413   1.0660404   2.5957007  -0.7834279  -0.08392825]...\n",
      "Вектор слова 'кофе': [-0.8308413   1.0660404   2.5957007  -0.7834279  -0.08392825]...\n",
      "Похожие слова на полученный вектор: [('кофе', 0.913850724697113), ('молоко', 0.8812078237533569), ('американо', 0.75205397605896)]\n"
     ]
    }
   ],
   "source": [
    "# Получение вектора для слова\n",
    "word_vector1 = model.wv[\"молоко\"]\n",
    "print(f\"Вектор слова 'молоко': {word_vector[:5]}...\")  # Вывод первых 5 значений\n",
    "\n",
    "word_vector2 = model.wv[\"кофе\"]\n",
    "print(f\"Вектор слова 'кофе': {word_vector[:5]}...\")  # Вывод первых 5 значений\n",
    "\n",
    "\n",
    "# Поиск похожих слов\n",
    "similar_words = model.wv.most_similar(word_vector1+word_vector2, topn=3)\n",
    "print(\"Похожие слова на полученный вектор:\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cffccb-0cb3-4271-a4cc-4a24bd1aa10d",
   "metadata": {},
   "source": [
    "Усреднение векторов слов для каждого документа в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "487e7c26-dcb4-40c3-b3b2-0a474e0ee6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(model, words):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200186-e45c-472e-83df-422a531a7c07",
   "metadata": {},
   "source": [
    "Проверяем как сработало усреднение на близких словах к полученному для каждого документа вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0484c14f-7188-4d60-a2ae-9343bce1e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors_word2vec = [text_to_vector(model, words) for words in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e40a4bb-e52d-4f86-8a35-34c572320e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['апрель', 'пройти', 'акция', 'монетный', 'неделя', 'россиянин', 'смочь', 'бесплатно', 'обменять', 'монета', 'банкнота', 'зачислять', 'сумма', 'счет', 'сообщать', 'центробанк', 'обмен', 'доступный', 'тыс', 'отделение', 'банк', 'тыс', 'торговый', 'точка', 'весь', 'страна']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('акция', 0.6212311387062073),\n",
       " ('платеж', 0.5912837386131287),\n",
       " ('банк', 0.5807708501815796),\n",
       " ('тыс', 0.5740810036659241),\n",
       " ('торги', 0.5729284882545471)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_test[2])\n",
    "model.wv.most_similar(data_vectors_word2vec[2], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ba8ba3e-2999-439d-890c-83ddf48a56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['искать', 'молодежный', 'тариф', 'игра', 'музыка', 'рестик', 'мтс', 'прогрессор', 'помощь', 'это', 'единый', 'хаб', 'развлечение', 'отдых', 'скачивать', 'приложение', 'рубиться', 'игра', 'получать', 'электронный', 'валюта', 'прокоин', 'смочь', 'поменять', 'скидка', 'промокод', 'пакет', 'интернет', 'пополнять', 'другой', 'сервис', 'проголодаться', 'личный', 'подборка', 'редакция', 'ия', 'подсказывать', 'вкусный', 'рестик', 'неподалеку', 'весь', 'это', 'также', 'гб', 'интернет', 'безлимит', 'телега', 'минута', 'смс', 'остаток', 'переноситься', 'подписка', 'мтс', 'музыка', 'конец', 'год', 'рубль', 'месяц', 'пока', 'москва', 'область', 'скоро', 'весь', 'страна', 'подробность']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('яндекс', 0.6557922959327698),\n",
       " ('бесплатно', 0.6203290820121765),\n",
       " ('сервис', 0.6081401109695435),\n",
       " ('подписка', 0.5977248549461365),\n",
       " ('бесплатный', 0.5748127698898315)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_test[5])\n",
    "model.wv.most_similar(data_vectors_word2vec[5], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f07e1-0a02-4a3c-94c4-d8df8479017c",
   "metadata": {},
   "source": [
    "### 2.3.2 Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54343a97-073b-41b0-a0e1-c08b9bb88c63",
   "metadata": {},
   "source": [
    "Перед использованием была изучена эта [статья].(https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e).\n",
    "\n",
    "Про отличие работы Word2Vec и Doc2Vec:\n",
    "\n",
    "The model above is called Distributed Memory version of Paragraph Vector (PV-DM). It acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "\n",
    "Here, this algorithm is actually faster (as opposed to word2vec) and consumes less memory, since there is no need to save the word vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc074904-acd4-4031-8d89-836d30e9211e",
   "metadata": {},
   "source": [
    " Пример использования из https://radimrehurek.com/gensim/models/doc2vec.html :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1396ab12-e8ba-43a7-ba1f-59fe719b9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00079551 -0.00393487  0.00413172 -0.00115991 -0.0082423  -0.00179026\n",
      " -0.00620833  0.00801841 -0.00428335 -0.00078557  0.00610221 -0.00211225\n",
      "  0.00655132  0.0066864  -0.00832772 -0.01053418 -0.00810287 -0.00533939\n",
      " -0.01030794 -0.00582843  0.00685461 -0.00220569 -0.00941168  0.00179322\n",
      "  0.00120937  0.00711207 -0.00242693  0.00938532  0.0083763  -0.00445437\n",
      " -0.00070403 -0.0039145  -0.00481523  0.00379924 -0.00446129 -0.000504\n",
      " -0.0035168   0.00810844  0.00283994  0.00127982  0.00300739  0.0077778\n",
      " -0.00464935  0.00853151 -0.0032797   0.00253399 -0.0073779  -0.00228772\n",
      "  0.00764174  0.00193785]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Пример данных\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Подготовка данных (токенизация и тегирование)\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) \n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "# Обучение модели\n",
    "model = Doc2Vec(\n",
    "    vector_size=50,  # Размер вектора\n",
    "    min_count=2,     # Минимальная частота слова\n",
    "    epochs=40        # Количество эпох\n",
    ")\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Получение вектора документа\n",
    "vector = model.infer_vector(word_tokenize(\"This is a new document.\"))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996f508-2ec6-48ac-ad58-8e589717cc7d",
   "metadata": {},
   "source": [
    "Наш датасет:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342085fc-4df0-487b-9954-2bb87012e331",
   "metadata": {},
   "source": [
    "Размер вектора для документа - **100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cf65d87d-5b39-4431-a2e9-ae0badadb804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.33696972e-02  3.45010370e-01 -2.41773099e-01 -7.69845545e-02\n",
      " -6.87041804e-02 -4.70700711e-01 -1.27060354e-01  3.67884487e-01\n",
      " -1.66482568e-01 -1.90216929e-01  3.35039757e-03 -5.47317863e-01\n",
      "  7.69934952e-02 -7.12446589e-03  1.97469756e-01 -2.57678628e-01\n",
      "  2.80142695e-01 -4.12734777e-01  2.25550890e-01 -1.79077849e-01\n",
      " -6.12685233e-02 -9.92543921e-02  2.09582001e-01  1.12087145e-01\n",
      "  6.91033244e-01  1.32431731e-01 -2.17868686e-01  3.07495985e-02\n",
      " -4.26150769e-01 -7.83714950e-02  5.96661642e-02 -1.94681644e-01\n",
      "  1.70603376e-02 -2.26324797e-01 -3.89791012e-01  5.80865443e-01\n",
      "  1.49893165e-01 -6.50546774e-02 -2.63585478e-01 -9.76955965e-02\n",
      " -4.38415527e-01 -9.58024785e-02  1.56487986e-01 -2.13762477e-01\n",
      " -1.56079948e-01 -2.80722044e-02 -9.43724066e-02 -1.52158663e-01\n",
      " -3.20958148e-04  3.46911907e-01 -1.29255921e-01 -4.76593003e-02\n",
      "  1.84030116e-01  1.39139742e-01  2.53484156e-02 -1.10862516e-01\n",
      " -7.12531060e-02 -2.19537511e-01  9.06086192e-02 -1.75159305e-01\n",
      " -4.51400243e-02  1.77634940e-01 -3.67028803e-01 -6.51728660e-02\n",
      " -2.11283952e-01  1.63610250e-01  1.89408675e-01  2.66091704e-01\n",
      " -3.14562172e-01  1.63174793e-01 -2.52417028e-01  1.69477060e-01\n",
      "  1.81333303e-01 -5.45103196e-03 -1.88440859e-01  3.64227384e-01\n",
      " -1.01285994e-01  3.28588426e-01 -1.39895916e-01  1.53466716e-01\n",
      " -1.73682243e-01  9.98377874e-02  5.53688519e-02  7.30286911e-02\n",
      " -1.15293510e-01 -1.22505873e-01 -3.17504883e-01  3.29564661e-01\n",
      "  2.95160979e-01 -2.62914360e-01  3.27037275e-02  2.61810154e-01\n",
      "  6.15646429e-02 -6.79545244e-03 -9.77464579e-03 -2.40997616e-02\n",
      " -2.34559048e-02 -3.27852964e-01  2.37947255e-01  1.81759268e-01]\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных (токенизация и тегирование)\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=words, tags=[str(i)]) \n",
    "    for i, words in enumerate(data_test)\n",
    "]\n",
    "\n",
    "# Обучение модели\n",
    "model = Doc2Vec(\n",
    "    vector_size=100,  # Размер вектора\n",
    "    min_count=2,     # Минимальная частота слова\n",
    "    epochs=40        # Количество эпох\n",
    ")\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Получение вектора документа\n",
    "vector = model.infer_vector(word_tokenize(\"Это еще один документ\"))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ea39c4dc-83a6-4c75-898e-5157dc036ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст \"0\" ['ученый', 'который', 'совершать', 'революция', 'лечение', 'рассеянный', 'склероз', 'получать', 'научный', 'оскар']\n",
      "\n",
      "Похожие на текст \"0\" по смыслу тексты:\n",
      "Текст \"15556\"\n",
      "['астроном', 'впервые', 'получать', 'портрет', 'звезда', 'галактика']\n",
      "Текст \"7152\"\n",
      "['вестись', 'эксперимент', 'внедрять', 'ген', 'мышь', 'получать', 'волосатый', 'мамонтовый', 'мышь']\n",
      "Текст \"1241\"\n",
      "['большой', 'розыгрыш', 'продолжаться', 'выполнять', 'условие', 'розыгрыш', 'получать', 'возможность', 'выигрывать', 'квартира', 'машина', 'тысяча', 'другой', 'приз', 'выбирать', 'любимый', 'направление', 'рейс', 'россия', 'санктпетербург', 'москва', 'москва', 'уфа', 'казань', 'москва', 'москва', 'ульяновск', 'сочи', 'самара', 'екатеринбург', 'сочи', 'международный', 'рейс', 'москва', 'гюмри', 'волгоград', 'стамбул', 'владикавказ', 'стамбул', 'москва', 'абудаби', 'махачкала', 'стамбул', 'москва', 'ош', 'цена', 'актуальный', 'момент', 'публикация', 'успевать', 'приобретать', 'приложение']\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.dv.most_similar('0', topn=3)  # Ищем документы, похожие на документ с тегом '0'\n",
    "\n",
    "similar_indexes = [int(tag) for tag, _ in similar_docs]\n",
    "print(f'Текст \"0\" {data_test[0]}')\n",
    "print()\n",
    "print('Похожие на текст \"0\" по смыслу тексты:')\n",
    "for similar in similar_indexes:\n",
    "    print(f'Текст \"{similar}\"\\n{data_test[similar]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "842d08ac-920e-45b1-b204-d6e623993aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст \"5\" ['великий', 'император', 'эфир', 'матч', 'тв', 'интервью', 'сам', 'франческо', 'тотти', 'легендарный', 'римлянин', 'приезжать', 'церемония', 'премия', 'рб', 'который', 'пройти', 'завтра', 'заглядывать', 'мы', 'гость', 'вопрос', 'черданцев', 'ответ', 'тотти', 'ловить', 'эфир', 'матч']\n",
      "\n",
      "Похожие на текст \"5\" по смыслу тексты:\n",
      "Текст \"11557\"\n",
      "['разыгрывать', 'рубль', 'ответ', 'вопрос', 'мой', 'вещь', 'сделать', 'игра', 'ребенок', 'мой', 'вещь', 'ребята', 'искать', 'предмет', 'картинка', 'забирать', 'подарок', 'предмет', 'спрятать', 'сюрприз', 'рубль', 'деньги', 'беспроводный', 'наушник', 'колонка', 'игровой', 'мышка', 'портативный', 'зарядка', 'еще', 'подсказка', 'пользоваться', 'детский', 'карта', 'копить', 'тратить', 'карманный', 'деньги', 'искать', 'предмет', 'сюрприз', 'детский', 'приложение', 'альфаонлайн', 'март', 'ребенок', 'пока', 'детский', 'карта', 'открывать', 'она', 'смочь', 'каждый', 'день', 'играть', 'вместе']\n",
      "Текст \"12287\"\n",
      "['сбертех', 'запускать', 'облачный', 'среда', 'разработка', 'работать', 'браузер', 'поддерживать', 'редактирование', 'отладка', 'совместный', 'работа', 'код', 'также', 'доступный', 'любой', 'устройство', 'среда', 'оснащать', 'ассистент', 'упрощать', 'рутинный', 'задача', 'мастхэв', 'инструмент', 'айтишник', 'начинать', 'пользоваться', 'ссылка']\n",
      "Текст \"167\"\n",
      "['секунда', 'бесплатно', 'подключаться', 'любой', 'точка', 'россия', 'первый', 'среди', 'банк', 'запускать', 'подключение', 'интернет', 'искать', 'сеть', 'весь', 'страна', 'аэропорт', 'метро', 'университет', 'поликлиника', 'нужно', 'ждать', 'проверочный', 'звонок', 'код', 'мошенник', 'смочь', 'перехватывать', 'ваш', 'пароль', 'личный', 'данные', 'бесплатно', 'безопасно', 'пользоваться', 'карта', 'альфабанка', 'подключаться', 'сеть', 'любой', 'город', 'всплывать', 'окно', 'нажимать', 'входить', 'следовать', 'инструкция', 'экран', 'редактор', 'канал', 'сей', 'пора', 'уверять', 'фишинг', 'это', 'рыбалка', 'кука', 'печенька']\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.dv.most_similar('5', topn=3)\n",
    "\n",
    "similar_indexes = [int(tag) for tag, _ in similar_docs]\n",
    "print(f'Текст \"5\" {data_test[1]}')\n",
    "print()\n",
    "print('Похожие на текст \"5\" по смыслу тексты:')\n",
    "for similar in similar_indexes:\n",
    "    print(f'Текст \"{similar}\"\\n{data_test[similar]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46108625-5530-4aae-aa4f-54c82f509cf4",
   "metadata": {},
   "source": [
    "Размер вектора для документа - **50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "180d4b48-67fb-4f47-ba49-24d87c5e45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1122893  -0.08361123  0.12109993 -0.0019567  -0.15847522  0.19078478\n",
      " -0.03695316  0.66916144 -0.27370995 -0.09069169  0.02462783  0.017155\n",
      "  0.01840624 -0.28219324  0.1917226   0.31793714  0.2182881  -0.2800033\n",
      " -0.11948295 -0.3417047  -0.00791297  0.53922445  0.03335609 -0.08313597\n",
      " -0.36942282  0.0285593   0.16309723  0.07219774  0.21331719 -0.07331865\n",
      " -0.20728905 -0.44554752 -0.29132205 -0.12919763 -0.5092853   0.02523815\n",
      " -0.20494033 -0.30540735  0.44927287 -0.22944018  0.2563658  -0.00779583\n",
      " -0.32665238  0.22720411  1.0809518   0.31136936 -0.15323918 -0.01887738\n",
      "  0.7372884   0.3220741 ]\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных (токенизация и тегирование)\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=words, tags=[str(i)]) \n",
    "    for i, words in enumerate(data_test)\n",
    "]\n",
    "\n",
    "# Обучение модели\n",
    "model = Doc2Vec(\n",
    "    vector_size=50,  # Размер вектора\n",
    "    min_count=2,     # Минимальная частота слова\n",
    "    epochs=40        # Количество эпох\n",
    ")\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Получение вектора документа\n",
    "vector = model.infer_vector(word_tokenize(\"Это еще один документ\"))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2c5fff74-4df2-40a2-81b5-1382ff4d5343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст \"0\" ['ученый', 'который', 'совершать', 'революция', 'лечение', 'рассеянный', 'склероз', 'получать', 'научный', 'оскар']\n",
      "\n",
      "Похожие на текст \"0\" по смыслу тексты:\n",
      "Текст \"7152\"\n",
      "['вестись', 'эксперимент', 'внедрять', 'ген', 'мышь', 'получать', 'волосатый', 'мамонтовый', 'мышь']\n",
      "Текст \"6278\"\n",
      "['клонирование', 'метод', 'получение', 'несколько', 'идентичный', 'организм', 'путем', 'бесполый', 'число', 'вегетативный', 'размножение', 'клонирование', 'зависимость', 'применение', 'группа', 'живой', 'неживой', 'существо', 'молекулярный', 'репродуктивный', 'терапевтический', 'клонирование', 'мочь', 'подвергаться', 'животное', 'человек', 'группа', 'генетически', 'идентичный', 'организм', 'клетка', 'называться', 'клон', 'термин', 'клонирование', 'приходить', 'русский', 'язык', 'английский', 'первоначально', 'слово', 'клон', 'дргреча', 'веточка', 'побег', 'отпрыск', 'становиться', 'употреблять', 'группа', 'растение', 'получать', 'один', 'растенияпроизводитель', 'вегетативный', 'способ', 'растенияпотомок', 'точность', 'повторять', 'качество', 'свой', 'прародитель', 'служить', 'основание', 'выведение', 'новый', 'сорт', 'клонирование', 'исходный', 'организм', 'клетка', 'служить', 'родоначальник', 'клон', 'ряд', 'организм', 'клетка', 'повторять', 'генотип', 'признак', 'родоначальник', 'поколение', 'поколение', 'такой', 'образ', 'сущность', 'клонирование', 'заключаться', 'повторение', 'один', 'тот', 'генетический', 'информация', 'особый', 'интерес', 'вызывать', 'эксперимент', 'связанный', 'клонирование', 'позвоночный', 'животное', 'человек', 'год', 'отечественный', 'ученый', 'пущинский', 'научный', 'центр', 'ран', 'впервые', 'осуществлять', 'клонирование', 'млекопитающее', 'мышь', 'яйцеклетка', 'удалять', 'ядро', 'затем', 'вводить', 'яйцеклетка', 'ядро', 'эмбриональный', 'мышиный', 'клетка', 'год', 'ученый', 'удаваться', 'клонировать', 'овца', 'долли', 'использовать', 'качество', 'донор', 'генетический', 'материал', 'эпителиальный', 'клетка', 'молочный', 'железа', 'эксперимент', 'поставлять', 'ян', 'вилмут', 'кит', 'кэмпбелл', 'рослинский', 'институт', 'шотландия', 'год', 'зародыш', 'вводить', 'организм', 'приемная', 'мать', 'который', 'вынашивать', 'ягненок', 'такой', 'образ', 'эксперимент', 'доказывать', 'получать', 'генетически', 'идентичный', 'копия', 'клон', 'млекопитающее', 'использовать', 'соматический', 'клетка', 'клетка', 'составлять', 'тело', 'многоклеточный', 'организм', 'принимать', 'участие', 'половой', 'размножение', 'эксперимент', 'клонирование', 'человек', 'осуждаться', 'международный', 'организация', 'запрещать', 'ряд', 'страна', 'неприемлемый', 'нравственный', 'отношение']\n",
      "Текст \"2323\"\n",
      "['год', 'специалист', 'планировать', 'отправлять', 'титан', 'космический', 'аппарат', 'изучение', 'поверхность', 'крупный', 'спутник', 'сатурн', 'команда', 'американский', 'инженер', 'помощь', 'компьютерный', 'моделирование', 'изучать', 'процесс', 'происходить', 'титан', 'выявлять', 'серьезный', 'опасность', 'который', 'поставлять', 'миссия', 'угроза']\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.dv.most_similar('0', topn=3)  # Ищем документы, похожие на документ с тегом '0'\n",
    "\n",
    "similar_indexes = [int(tag) for tag, _ in similar_docs]\n",
    "print(f'Текст \"0\" {data_test[0]}')\n",
    "print()\n",
    "print('Похожие на текст \"0\" по смыслу тексты:')\n",
    "for similar in similar_indexes:\n",
    "    print(f'Текст \"{similar}\"\\n{data_test[similar]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f206c329-91af-4a3c-85e8-03528d139f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст \"5\" ['искать', 'молодежный', 'тариф', 'игра', 'музыка', 'рестик', 'мтс', 'прогрессор', 'помощь', 'это', 'единый', 'хаб', 'развлечение', 'отдых', 'скачивать', 'приложение', 'рубиться', 'игра', 'получать', 'электронный', 'валюта', 'прокоин', 'смочь', 'поменять', 'скидка', 'промокод', 'пакет', 'интернет', 'пополнять', 'другой', 'сервис', 'проголодаться', 'личный', 'подборка', 'редакция', 'ия', 'подсказывать', 'вкусный', 'рестик', 'неподалеку', 'весь', 'это', 'также', 'гб', 'интернет', 'безлимит', 'телега', 'минута', 'смс', 'остаток', 'переноситься', 'подписка', 'мтс', 'музыка', 'конец', 'год', 'рубль', 'месяц', 'пока', 'москва', 'область', 'скоро', 'весь', 'страна', 'подробность']\n",
      "\n",
      "Похожие на текст \"5\" по смыслу тексты:\n",
      "Текст \"10099\"\n",
      "['премиальный', 'карта', 'газпромбанк', 'сочетать', 'комфорт', 'привилегия', 'выгода', 'год', 'газпромбанк', 'премиум', 'получать', 'привилегия', 'спорт', 'путешествие', 'выбор', 'менять', 'каждый', 'месяц', 'кешбэк', 'рубль', 'миля', 'повышенный', 'доход', 'накопление', 'бизнесзал', 'трансфер', 'фитнес', 'спа', 'бесплатный', 'мобильный', 'связь', 'эксклюзивный', 'предложение', 'партнер', 'банк', 'все', 'включать', 'хороший', 'премиальный', 'карта', 'результат', 'исследование', 'россия', 'компания']\n",
      "Текст \"8556\"\n",
      "['секретный', 'коллаб', 'магнит', 'официально', 'начинаться', 'приключенец', 'мочь', 'выдвигаться', 'март', 'апрель', 'покупка', 'карта', 'магнит', 'плюс', 'ивент', 'приключение', 'год', 'магнит', 'получать', 'фигурка', 'брелок', 'промокод', 'приложение', 'магнит', 'быть', 'скидочный', 'купон', 'внутриигровой', 'бонус', 'прохождение', 'спецуровень', 'приложение', 'выигрывать', 'приз', 'участвовать', 'розыгрыш', 'мерч', 'смартфон', 'другой', 'награда', 'ожидание', 'большой', 'обновление', 'скрасить', 'подробность', 'акция', 'приложение', 'магнит', 'реклама', 'ао', 'тандер', 'инн']\n",
      "Текст \"6132\"\n",
      "['вчерашинй', 'пост', 'собирать', 'коммент', 'написать', 'пошаговый', 'инструкция', 'оформлять', 'зарубежный', 'карта', 'бесплатно', 'регистрация', 'смс', 'еще', 'обо', 'бонус', 'рассказывать', 'суммарно', 'получать', 'бакс', 'привычный', 'кешбек', 'карта']\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.dv.most_similar('5', topn=3)\n",
    "\n",
    "similar_indexes = [int(tag) for tag, _ in similar_docs]\n",
    "print(f'Текст \"5\" {data_test[5]}')\n",
    "print()\n",
    "print('Похожие на текст \"5\" по смыслу тексты:')\n",
    "for similar in similar_indexes:\n",
    "    print(f'Текст \"{similar}\"\\n{data_test[similar]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bbbfa-de0a-4bc6-a4ce-d5f91f25baf4",
   "metadata": {},
   "source": [
    "### 2.3.3 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffced4f0-b4c0-4cae-8683-5a9bd5521bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\s.yanenkov\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\s.yanenkov\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Загрузка русской модели (пример пути)\n",
    "model_path = \"ru_glove_100d.txt\"\n",
    "glove_model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "# Корпус на русском\n",
    "documents = [\n",
    "    \"Привет, как твои дела?\",\n",
    "    \"Машинное обучение — это интересно.\",\n",
    "    \"Глубокое обучение использует нейронные сети.\"\n",
    "]\n",
    "\n",
    "# Токенизация + лемматизация (опционально)\n",
    "tokenized_docs = [word_tokenize(doc.lower(), language=\"russian\") for doc in documents]\n",
    "\n",
    "# Функция для усреднения векторов\n",
    "def doc_to_vector(tokens, model):\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Векторизация\n",
    "doc_vectors = np.array([doc_to_vector(doc, glove_model) for doc in tokenized_docs])\n",
    "print(f\"Векторы документов: {doc_vectors.shape}\")  # (3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d147b0cf-8382-44a4-8988-83e4e72b807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\s.yanenkov\\appdata\\local\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\s.yanenkov\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\s.yanenkov\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\s.yanenkov\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3099baaf-633e-44f2-b56c-6c9864139865",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1356\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\_mmreader.pyx:11\u001b[0m, in \u001b[0;36minit gensim.corpora._mmreader\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name utils",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. "
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d04e35-67bd-4f9b-9030-88aa50b4f253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1e4db-f703-4d67-a634-4397e2f49a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0524d-aee4-4e61-9d4d-734e2ec3b471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d0d5a-40af-47b5-808c-7486d6dd68ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca3d5cd9-a9a2-406d-9375-1ca8c2133821",
   "metadata": {},
   "source": [
    "### 2.3.4 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe315cb-f5ca-4a91-a49d-11b00edbb4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
